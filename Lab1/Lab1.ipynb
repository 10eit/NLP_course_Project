{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVpKkBI6p0h2"
   },
   "source": [
    "# Project 1 BiLSTM-based NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRm8-0-oRZTz"
   },
   "source": [
    "print out current `torch` and `torchtext` version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75WPvPU3ReDt",
    "outputId": "85824cea-d5f0-4d9c-9317-7d197ab6ac51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n",
      "0.18.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "#make it reproducible, use fixed random seed\n",
    "torch.manual_seed(42)\n",
    "print(torch.__version__)\n",
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this course doesn't offer any GPU resource, so I buy some points on AutoDL... Check my Plan first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEof891Hp0iD"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tYP3o9Lhp0iE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CHisIECDataset(Dataset):\n",
    "    label_label_id_mapping = {\n",
    "        \"O\": 0,\n",
    "        \"B-PER\": 1,\n",
    "        \"I-PER\": 2,\n",
    "        \"E-PER\": 3,\n",
    "        \"S-PER\": 4,\n",
    "        \"B-LOC\": 5,\n",
    "        \"I-LOC\": 6,\n",
    "        \"E-LOC\": 7,\n",
    "        \"S-LOC\": 8,\n",
    "        \"B-OFI\": 9,\n",
    "        \"I-OFI\": 10,\n",
    "        \"E-OFI\": 11,\n",
    "        \"S-OFI\": 12,\n",
    "        \"B-BOOK\": 13,\n",
    "        \"I-BOOK\": 14,\n",
    "        \"E-BOOK\": 15,\n",
    "        \"S-BOOK\": 16,\n",
    "    }\n",
    "\n",
    "    def __init__(self, path) -> None:\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            d = [[], []]\n",
    "            while line := f.readline():\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    word, label = line.split()\n",
    "                    d[0].append(word)\n",
    "                    d[1].append(self.label_label_id_mapping[label])\n",
    "                elif d[0]:\n",
    "                    self.data.append(tuple(d))\n",
    "                    d = [[], []]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qN_Tjy68d_ah"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "train_set = CHisIECDataset(\"./CHisIEC/train.txt\")\n",
    "dev_set = CHisIECDataset(\"./CHisIEC/dev.txt\")\n",
    "test_set = CHisIECDataset(\"./CHisIEC/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52Q2Ob1Bp0iH"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC0EeQ9GVML8"
   },
   "source": [
    "Below Cell defines the default network structure from the TAs, the network structure consists of\n",
    "\n",
    "Embedding->Bidirectional LSTM -> MLP Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnkmUbsPp0iH",
    "outputId": "6a3de1ef-ab84-4b41-b09d-0b5075125f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 17])\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "from torch import nn\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "\n",
    "from torch.nn import LSTM\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyAwesomeModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=50, hidden_dim=50) -> None:\n",
    "        super().__init__()\n",
    "        self.vectors = Vectors(\n",
    "            name=\"gigaword_chn.all.a2b.uni.ite50.vec\",\n",
    "            cache=\".\",\n",
    "        )\n",
    "        self.lstm = LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 17)\n",
    "\n",
    "    def forward(self, x: str):\n",
    "        x = self.vectors.get_vecs_by_tokens(x).to(\"cuda\")\n",
    "        x, _ = self.lstm(x.unsqueeze(0))\n",
    "        x = self.classifier(x[0])\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = MyAwesomeModel().cuda()\n",
    "tokens = [\"hello\", \"world\"]\n",
    "logits = model(tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOcrGPVpVRRh"
   },
   "source": [
    "We will test another model that add attention after the BiLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTYXBZq-V1RN",
    "outputId": "567ae0a1-44b8-4c89-f5b2-beb197368086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 17])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import LSTM\n",
    "\n",
    "class MyAwesomeModel_withAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=50, hidden_dim=50,attention_heads=5,num_classes=17) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pre-trained vectors\n",
    "        self.vectors = Vectors(\n",
    "            name=\"gigaword_chn.all.a2b.uni.ite50.vec\",\n",
    "            cache=\".\",\n",
    "        )\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim * 2, num_heads=attention_heads, batch_first=True)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        self.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def forward(self, tokens: list):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        tokens: A list of tokenized words (strings).\n",
    "        \"\"\"\n",
    "        # Get word embeddings for the tokens\n",
    "        embeddings = self.vectors.get_vecs_by_tokens(tokens)  # shape: (seq_len, embed_dim)\n",
    "\n",
    "        # Move embeddings to the correct device (e.g., 'cuda')\n",
    "        embeddings = embeddings.to(self.classifier.weight.device)  # Use the same device as the classifier\n",
    "\n",
    "        # Add batch dimension (batch_size=1 for a single sequence)\n",
    "        embeddings = embeddings.unsqueeze(0)  # shape: (1, seq_len, embed_dim)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embeddings)  # Output: (batch_size, num_of_tokens, hidden_size * 2)\n",
    "\n",
    "        # Apply self-attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)  # Shape stays the same: (batch_size, num_of_tokens, hidden_size * 2)\n",
    "\n",
    "        # Token-wise classification\n",
    "        logits = self.classifier(attn_out)  # Output: (batch_size, num_of_tokens, num_classes)\n",
    "\n",
    "        return logits[0]\n",
    "\n",
    "# Example usage\n",
    "model = MyAwesomeModel_withAttention().cuda()\n",
    "tokens = [\"hello\", \"world\"]\n",
    "logits = model(tokens)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSyn9NbAp0iK"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn968EJfX_zP"
   },
   "source": [
    "In order to simplify the notebook structure, pack up all training and testing process into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bJaejfRCX5fn"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import trange\n",
    "\n",
    "def train(model,metric_fn,optimizer,loader):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        label = y.to(\"cuda\")\n",
    "        try:\n",
    "            loss = metric_fn(pred, label)\n",
    "        except:\n",
    "            print(pred.shape, label.shape)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return {\"loss\": sum(epoch_loss) / len(epoch_loss)}\n",
    "\n",
    "def eval(model,loader):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    target = []\n",
    "    for x, y in loader:\n",
    "        _pred = model(x).argmax(-1)\n",
    "        pred += _pred.tolist()\n",
    "        _target = y.argmax(-1)\n",
    "        target += _target.tolist()\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(target, pred),\n",
    "        \"f1_macro\": f1_score(target, pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "def training_and_testing(model,metric_fn,optimizer,epochs,train_loader,val_loader):\n",
    "    metrics_list = list()\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "        metrics = train(model,metric_fn,optimizer,loader=train_loader)\n",
    "        with torch.no_grad():\n",
    "            metrics = {**eval(model,loader=val_loader), **metrics}\n",
    "            print(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "    return metrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNv6yQr4XFHa"
   },
   "source": [
    "Training on default models. According to the [paper](https://proceedings.neurips.cc/paper_files/paper/2019/hash/dc6a70712a252123c40d2adba6a11d84-Abstract.html), the ratio of the learning rate and batch size is larger, the better the generalization, we make this principle our guidance in this project(I make a silly error and we need to re-train this model again, luckily we fixed the random seed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [03:04<00:00, 18.49s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:07<00:00, 18.72s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:07<00:00, 18.73s/it]\n",
      "Epoch: 100%|██████████| 10/10 [01:34<00:00,  9.41s/it]\n",
      "Epoch: 100%|██████████| 10/10 [01:33<00:00,  9.35s/it]\n",
      "Epoch: 100%|██████████| 10/10 [01:32<00:00,  9.26s/it]\n",
      "Epoch: 100%|██████████| 10/10 [00:46<00:00,  4.70s/it]\n",
      "Epoch: 100%|██████████| 10/10 [00:45<00:00,  4.59s/it]\n",
      "Epoch: 100%|██████████| 10/10 [00:46<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'batch_size': [1,2,4],\n",
    "    'lr': [1e-4,1e-3,1e-2],\n",
    "}\n",
    "\n",
    "## previous searched best params 'epochs': 10, 'hidden_size': 256, and then my kernel crash...\n",
    "epochs = 10\n",
    "hidden_size = 256\n",
    "\n",
    "# Generate Parameters Combination\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Record Best HyperParameters\n",
    "best_f1 = float('-inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Open a file to write the output\n",
    "with open('BiLSTM_training_output.txt', 'w') as f:\n",
    "    # Redirect stdout to the file\n",
    "    import sys\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = f\n",
    "\n",
    "    # Iterate on all Parameter combination\n",
    "    for params in param_combinations:\n",
    "        print(f\"Training with params: {params}\")\n",
    "\n",
    "        def get_dataloader(dataset, shuffle=True):\n",
    "            def collect_fn(batch):\n",
    "                t = batch[0][0]\n",
    "                l = one_hot(torch.tensor(batch[0][1], dtype=torch.int64), 17).float()\n",
    "                return t, l\n",
    "\n",
    "            return DataLoader(\n",
    "                dataset,\n",
    "                shuffle=shuffle,\n",
    "                batch_size=params['batch_size'],\n",
    "                collate_fn=collect_fn,\n",
    "            )\n",
    "\n",
    "        train_loader = get_dataloader(train_set)\n",
    "        val_loader = get_dataloader(dev_set, shuffle=False)\n",
    "\n",
    "        # Initialize Model Every Time!!\n",
    "        model = MyAwesomeModel(hidden_dim=hidden_size).cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "        # Train and Eval Models\n",
    "        metrics_list = training_and_testing(model, metric_fn=torch.nn.CrossEntropyLoss(), optimizer=optimizer, epochs=epochs,train_loader=train_loader,val_loader=val_loader)\n",
    "\n",
    "        # Get Final Results\n",
    "        final_metrics = metrics_list[-1]\n",
    "        f1_macro = final_metrics['f1_macro']\n",
    "\n",
    "        # Updating Params\n",
    "        if f1_macro > best_f1:\n",
    "            best_f1 = f1_macro\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Current Best F1 Macro: {best_f1}\")\n",
    "        print(f\"Best Params so far: {best_params}\")\n",
    "\n",
    "    print(f\"Best overall F1 Macro: {best_f1}\")\n",
    "    print(f\"Best Params: {best_params}\")\n",
    "\n",
    "    # Restore stdout\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best overall F1 Macro: 0.6349876246115874\n",
      "Best Params: {'batch_size': 1, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best overall F1 Macro: {best_f1}\")\n",
    "print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least we pass the criterion for getting 60 points in this project(the whole training process is in the output txt file), Congrats to myself! Let's check the parameter grid: \n",
    "|      | 1e-2   | 1e-3   | 1e-4   |\n",
    "| ---- | ------ | ------ | ------ |\n",
    "| 1    | 0.4418 | 0.6350 | 0.6113 |\n",
    "| 2    | 0.4042 | 0.6007 | 0.5583 |\n",
    "| 4    | 0.4371 | 0.5953 | 0.4348 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seem like our guiding paper(which might be a little bit old, on 2019 NIPS) was right, the higher the ratio, the better the generalization performance(within same learning rate). And the learning rate between 1e-3 and 1e-4 differs a little when in small batch but dramatic in larger batch size, we think it might come from the error accumulation in large batch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ot4EQ4oRaueg"
   },
   "source": [
    "Training the model with Multihead-Attention after BiLSTM layer. We shall use Multihead-Attention not Attention alone(or older textbook will refer it to something called **Global Alignment Weight**) since we need token-wise classification, Attention alone will returns the whole representation of the sentence(which is better for sentimental/title classification).\n",
    "\n",
    "As a newcomer in NLP and DL, I fall in the trap of using Attention alone at first and being frustrating about the output size...\n",
    "\n",
    "**Notice**: In this time we only grid search the attention heads and learning rate, and this time we will change params more precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "bgAXT54_XJcm",
    "outputId": "99e3aa8c-93e7-4afe-e2ac-5d5896cc3132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [03:43<00:00, 22.37s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:33<00:00, 21.34s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:33<00:00, 21.36s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:43<00:00, 22.34s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:29<00:00, 20.91s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:37<00:00, 21.75s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:39<00:00, 21.94s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:41<00:00, 22.12s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:26<00:00, 20.67s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:39<00:00, 21.99s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:35<00:00, 21.50s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:31<00:00, 21.14s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:42<00:00, 22.24s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:35<00:00, 21.57s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:33<00:00, 21.39s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:43<00:00, 22.35s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:35<00:00, 21.52s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:40<00:00, 22.07s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:44<00:00, 22.47s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:41<00:00, 22.14s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:36<00:00, 21.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "## previous searched best params 'epochs': 10, 'hidden_size': 256, and then my kernel crash...\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "hidden_size = 256\n",
    "\n",
    "## hiden size is 256, attention heads should be divisible!!!\n",
    "param_grid = {\n",
    "    'attention_heads': [2,4,8,16,32,64,128],\n",
    "    'lr': [1e-3,1.5 * 1e-3,8.5 * 1e-2],\n",
    "}\n",
    "\n",
    "# Generate Parameters Combination\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Record Best HyperParameters\n",
    "best_f1 = float('-inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Open a file to write the output\n",
    "with open('Multihead_attention_training_output.txt', 'w') as f:\n",
    "    # Redirect stdout to the file\n",
    "    import sys\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = f\n",
    "\n",
    "    # Iterate on all Parameter combination\n",
    "    for params in param_combinations:\n",
    "        print(f\"Training with params: {params}\")\n",
    "\n",
    "        def get_dataloader(dataset, shuffle=True):\n",
    "            def collect_fn(batch):\n",
    "                t = batch[0][0]\n",
    "                l = one_hot(torch.tensor(batch[0][1], dtype=torch.int64), 17).float()\n",
    "                return t, l\n",
    "\n",
    "            return DataLoader(\n",
    "                dataset,\n",
    "                shuffle=shuffle,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collect_fn,\n",
    "            )\n",
    "\n",
    "        train_loader = get_dataloader(train_set)\n",
    "        val_loader = get_dataloader(dev_set, shuffle=False)\n",
    "\n",
    "        # Initialize Model Every Time!!\n",
    "        model = MyAwesomeModel_withAttention(hidden_dim=hidden_size,attention_heads=params['attention_heads']).cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "        # Train and Eval Models\n",
    "        metrics_list = training_and_testing(model, metric_fn=torch.nn.CrossEntropyLoss(), optimizer=optimizer, epochs=epochs,train_loader=train_loader,val_loader=val_loader)\n",
    "\n",
    "        # Get Final Results\n",
    "        final_metrics = metrics_list[-1]\n",
    "        f1_macro = final_metrics['f1_macro']\n",
    "\n",
    "        # Updating Params\n",
    "        if f1_macro > best_f1:\n",
    "            best_f1 = f1_macro\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Current Best F1 Macro: {best_f1}\")\n",
    "        print(f\"Best Params so far: {best_params}\")\n",
    "\n",
    "    print(f\"Best overall F1 Macro: {best_f1}\")\n",
    "    print(f\"Best Params: {best_params}\")\n",
    "\n",
    "    # Restore stdout\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best overall F1 Macro: 0.5922000319627826\n",
      "Best Params: {'attention_heads': 128, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best overall F1 Macro: {best_f1}\")\n",
    "print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we need more attention head, let's add some more(Notice that in PyTorch's implementation, attention head should be divisible to the actual input hidden dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [03:35<00:00, 21.50s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:39<00:00, 21.98s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:34<00:00, 21.49s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:35<00:00, 21.59s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:43<00:00, 22.37s/it]\n",
      "Epoch: 100%|██████████| 10/10 [03:46<00:00, 22.63s/it]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "## previous searched best params 'epochs': 10, 'hidden_size': 256, and then my kernel crash...\n",
    "epochs = 10\n",
    "batch_size = 1\n",
    "hidden_size = 256\n",
    "\n",
    "## hiden size is 256, attention heads should be divisible!!!\n",
    "param_grid = {\n",
    "    'attention_heads': [256,512],\n",
    "    'lr': [1e-3,1.5 * 1e-3,8.5 * 1e-2],\n",
    "}\n",
    "\n",
    "# Generate Parameters Combination\n",
    "param_combinations = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Record Best HyperParameters\n",
    "best_f1 = float('-inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Open a file to write the output\n",
    "with open('Multihead_attention_training_output.txt', 'w') as f:\n",
    "    # Redirect stdout to the file\n",
    "    import sys\n",
    "    original_stdout = sys.stdout\n",
    "    sys.stdout = f\n",
    "\n",
    "    # Iterate on all Parameter combination\n",
    "    for params in param_combinations:\n",
    "        print(f\"Training with params: {params}\")\n",
    "\n",
    "        def get_dataloader(dataset, shuffle=True):\n",
    "            def collect_fn(batch):\n",
    "                t = batch[0][0]\n",
    "                l = one_hot(torch.tensor(batch[0][1], dtype=torch.int64), 17).float()\n",
    "                return t, l\n",
    "\n",
    "            return DataLoader(\n",
    "                dataset,\n",
    "                shuffle=shuffle,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collect_fn,\n",
    "            )\n",
    "\n",
    "        train_loader = get_dataloader(train_set)\n",
    "        val_loader = get_dataloader(dev_set, shuffle=False)\n",
    "\n",
    "        # Initialize Model Every Time!!\n",
    "        model = MyAwesomeModel_withAttention(hidden_dim=hidden_size,attention_heads=params['attention_heads']).cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "        # Train and Eval Models\n",
    "        metrics_list = training_and_testing(model, metric_fn=torch.nn.CrossEntropyLoss(), optimizer=optimizer, epochs=epochs,train_loader=train_loader,val_loader=val_loader)\n",
    "\n",
    "        # Get Final Results\n",
    "        final_metrics = metrics_list[-1]\n",
    "        f1_macro = final_metrics['f1_macro']\n",
    "\n",
    "        # Updating Params\n",
    "        if f1_macro > best_f1:\n",
    "            best_f1 = f1_macro\n",
    "            best_params = params\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"Current Best F1 Macro: {best_f1}\")\n",
    "        print(f\"Best Params so far: {best_params}\")\n",
    "\n",
    "    print(f\"Best overall F1 Macro: {best_f1}\")\n",
    "    print(f\"Best Params: {best_params}\")\n",
    "\n",
    "    # Restore stdout\n",
    "    sys.stdout = original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best overall F1 Macro: 0.6322233039655257\n",
      "Best Params: {'attention_heads': 512, 'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best overall F1 Macro: {best_f1}\")\n",
    "print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed learning rate to be 1e-3, we find the F1-marcos relations with head number to be(very sad due to code implementation trap, I overwrite all output from 2-128):\n",
    "\n",
    "| Head Number      | 2   | 4   | 8  | 16 | 32 | 64 | 128| 256 | 512 |\n",
    "| ---- | ------ | ------ | ------ | ----- | ----- | ----- | ----- | ----- | ----- |\n",
    "| F1-Marcos    | 0.5486 | 0.5139 | 0.5892 |0.5554 | 0.5645 | 0.5855|0.5922 |0.5997|0.6322|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the higher the head number, the better the F1-marcos we will get, intuitively, the more head number captures more last output layer representation temporally(as we input sequences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdANH_qlPx8j"
   },
   "source": [
    "## Model Training Tricks Effects on F1-Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the best params we find in previous section, we will try some classical tricks and see whether it will impove F1-Marcos. The first technique we try is the dropout method, in this project we mainly focus on BiLSTM-based structure, so we will fix some params we find best in previous grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 17])\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyAwesomeModel_withDropout(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=50, hidden_dim=256, dropout_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.vectors = vocab.Vectors(\n",
    "            name=\"gigaword_chn.all.a2b.uni.ite50.vec\",\n",
    "            cache=\".\",\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 17)\n",
    "\n",
    "    def forward(self, x: str):\n",
    "        x = self.vectors.get_vecs_by_tokens(x).to(\"cuda\")\n",
    "        x, _ = self.lstm(x.unsqueeze(0))\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x[0])\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = MyAwesomeModel_withDropout().to(\"cuda\")\n",
    "tokens = [\"hello\", \"world\"]\n",
    "logits = model(tokens)\n",
    "print(logits.shape)  # Output: (17,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WGol0PqkP5oU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [00:19<02:54, 19.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8847151218789361, 'f1_macro': 0.5095718896149861, 'loss': 0.595092751166882}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [00:38<02:35, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8978291472179003, 'f1_macro': 0.5826376577747796, 'loss': 0.30117418805195983}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [00:58<02:16, 19.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9030895754612136, 'f1_macro': 0.6269906174668747, 'loss': 0.19553928284380148}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [01:17<01:56, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9102763577091205, 'f1_macro': 0.6426782327286192, 'loss': 0.13272587810876516}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [01:37<01:37, 19.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9070904645476773, 'f1_macro': 0.6561510335519928, 'loss': 0.09386270173655538}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [01:56<01:17, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9082759131658887, 'f1_macro': 0.643973014011336, 'loss': 0.07295414712295045}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [02:15<00:57, 19.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9073868267022301, 'f1_macro': 0.610762705419249, 'loss': 0.05653008915432082}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [02:34<00:38, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9042009335407868, 'f1_macro': 0.6242944054492563, 'loss': 0.04921140797597976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [02:53<00:18, 18.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9096836334000148, 'f1_macro': 0.6284413694431114, 'loss': 0.04345034519752923}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [03:12<00:00, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9061272875453804, 'f1_macro': 0.626151644183956, 'loss': 0.037155030771844705}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "lr = 0.001\n",
    "epoch = 10\n",
    "\n",
    "def get_dataloader(dataset, shuffle=True):\n",
    "    def collect_fn(batch):\n",
    "        t = batch[0][0]\n",
    "        l = one_hot(torch.tensor(batch[0][1], dtype=torch.int64), 17).float()\n",
    "        return t, l\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collect_fn,\n",
    "    )\n",
    "\n",
    "train_loader = get_dataloader(train_set)\n",
    "val_loader = get_dataloader(dev_set, shuffle=False)\n",
    "\n",
    "# Initialize Model Every Time!!\n",
    "model = MyAwesomeModel_withDropout().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train and Eval Models\n",
    "metrics_list = training_and_testing(model, metric_fn=torch.nn.CrossEntropyLoss(), optimizer=optimizer, epochs=epochs,train_loader=train_loader,val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final F1 scores is: 0.626151644183956\n"
     ]
    }
   ],
   "source": [
    "print(\"The final F1 scores is:\",metrics_list[-1][\"f1_macro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the using dropout technique doesn't sota the previous model with best hyperparmeters, but if we see the training trace we will find that there is some epoch, the F1-score is significant higher than baseline. A proper explaination for this is that 0-encoded component is too many in the dataset, hence in optimization, it will be more likely to classify it as right(which reduce the loss but lower the F1-scores)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Theoretical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is crucial for this task, but it introduces a class imbalance problem with numerous zero labels. This can bias the optimizer towards predicting more zeros. \n",
    "\n",
    "While Non-sequence models like CNN might can improve performance, but they might not fully capture the syntactic nuances required for classification. They focus on identifying words likely to belong to specific parts but neglect sentence-level structural information.\n",
    "\n",
    "An idea that I have no time to try it:\n",
    "If we use tf-idf like method to calculate the different components(17 in whole) and use the frequency to encode different part, maybe we can reduce the tendency of making all prediction to zero(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk-44XT-p0iO"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJXLwKZBp0iP"
   },
   "source": [
    "In this project we do:\n",
    "* an BiLSTM-based NER classifier\n",
    "* BiLSTM+Multi-head Attention NER classifier\n",
    "* dropout Training technique was applied to the BiLSTM-based NER classifier\n",
    "  \n",
    "and we find that:\n",
    "* among different hidden dimension, learning rate, training epoch numbers and batch size, the best params was 256, 1e-3, 10 and 1 respectively and we will get best performance of F1-scores to be 0.6349.\n",
    "* when adding multi-head attention, with about 512 heads, we reach best performace of 0.6322.\n",
    "* dropout with 10% chance make the F1-scores to 0.6261.\n",
    "\n",
    "and we thinks that some problem can be tackled by:\n",
    "* using better trained embedding vectors\n",
    "* finding proper encoding methods other than one hot."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
